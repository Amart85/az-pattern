<properties
pageTitle= 'how to create two Azure hub-spoke VNets interconnected by global VNet peering by ARM'
description= "ARM template to create two Azure hub-spoke VNets interconnected by global VNet peering"
documentationcenter: na
services=""
documentationCenter="na"
authors="fabferri"
manager=""
editor=""/>

<tags
   ms.service="howto-example-Azure"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="26/07/2018"
   ms.author="fabferri" />

# How to create two hub-spoke VNets interconnected by global VNet peering
This ARM template aims to create two hub-spoke vnets in different Azure regions, with hub vnets interconnected through global vnet peering.


The network diagram is reported below:

[![1]][1]

To forward the traffic between spoke vnets is required nva1, nva2,  with ip forwarding enabled. The replacement of NVAs with simple linux VMs is useful for to troubleshooting to verify the traffic flows properly through the NVAs.
Internal load balancer with frontend IPs and backend pools is show above:

[![2]][2]


> [!NOTE]
> Before spinning up the ARM template you should:
> * set the Azure subscription name in the file **vnet-peering.ps1**
> * set the administrator username and password in the file **vnet-peering.json**
>


#### <a name="EnableIPForwarding"></a>1. Enable ip forwarding on nva1 and nva2

Enable ip forwarding in nv1 and nv2:

```
sed -i -e '$a\net.ipv4.ip_forward = 1' /etc/sysctl.conf
systemctl restart network.service
sysctl net.ipv4.ip_forward
```

#### <a name="Iperf3"></a>2. Generate traffic between vm3 and vm4
Traffic can be generated by **netcat** and **urandom** the random number function in the linux kernel.

Install netcat (nc) in vm3 and vm4:
```
[root@vm3 ~]# yum -y install nmap-ncat
[root@vm4 ~]# yum -y install nmap-ncat
```
write two bash scripts: one for the server (traffic receiver) and one for the client (traffic sender).

file: **server.sh**

```bash
#!/bin/bash
#
val=true
while [ $val ]
do
 nc -l -p 9000 > /dev/null 2>&1
 wait
done
```
file: **client.sh**

```bash
for i in {1..10};
do
  dd if=/dev/urandom bs=1M count=100 | nc 10.0.4.10 9000
  sleep 2
done
```
To send traffic from vm3 to vm4 run:

```console
[root@vm4 ~]#./server.sh
[root@vm4 ~]#./client.sh
```

By tcpdump check the transit through nva1 and nva2 in both directions: [from vm3 to vm4] AND [from vm4 to vm3]

```console
[root@nva1 ~]# tcpdump -nqt -i eth0 host 10.0.3.10
[root@nva2 ~]# tcpdump -nqt -i eth0 host 10.0.3.10
```

To monitor the traffic on vm3 and vm4 we can use a tool like **iftop**

```console
#yum -y install libpcap libpcap-devel ncurses ncurses-devel
#yum -y install epel-release
#yum -y install  iftop
```

<!--Image References-->

[1]: ./media/network-diagram.png "network diagram"
[2]: ./media/flow.png "tcp flow transit from vm2 to vm3"


<!--Link References-->

